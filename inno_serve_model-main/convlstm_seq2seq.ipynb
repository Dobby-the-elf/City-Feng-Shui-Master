{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/event\"\n",
    "DATA_DIR = path.abspath(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_images = []\n",
    "for fn in os.listdir(DATA_DIR):\n",
    "    date_slices = fn.split(\"_\")\n",
    "    \n",
    "    data_file = f\"{DATA_DIR}/{fn}/event.csv\"\n",
    "    df_np = pd.read_csv(data_file).to_numpy()\n",
    "\n",
    "    event1_numbers = np.append(df_np[:, 3], 0).reshape((1, 48, 63))\n",
    "    event2_numbers = np.append(df_np[:, 4], 0).reshape((1, 48, 63))\n",
    "    event3_numbers = np.append(df_np[:, 5], 0).reshape((1, 48, 63))\n",
    "    event4_numbers = np.append(df_np[:, 6], 0).reshape((1, 48, 63))\n",
    "\n",
    "    event_image = np.concatenate((event1_numbers, event2_numbers, event3_numbers, event4_numbers), axis = 0)\n",
    "    event_images.append(event_image)\n",
    "    \n",
    "event_images = torch.tensor(event_images, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([8760, 4, 48, 63])\n",
      "max: 22.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape: {event_images.shape}\")\n",
    "print(f\"max: {torch.max(event_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape every 120 hours (72 hours for encoder, 48 hours for decoder), the time window slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_WINDOW = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(examples): 360\n",
      "examples[0].shape: torch.Size([1, 120, 4, 48, 63])\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for i in range(0, event_images.shape[0] - TIME_WINDOW, 24):\n",
    "    examples.append(event_images[i:i+TIME_WINDOW].unsqueeze(0))\n",
    "\n",
    "print(f\"len(examples): {len(examples)}\")\n",
    "print(f\"examples[0].shape: {examples[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Training Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_ratio = 0.8 # 4/5 to be training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 299 examples\n",
      "testing set size: 61 examples\n"
     ]
    }
   ],
   "source": [
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for idx in range(len(examples)):\n",
    "    if random.random() <= train_test_ratio:\n",
    "        train_set.append(examples[idx])\n",
    "    else:\n",
    "        test_set.append(examples[idx])\n",
    "        \n",
    "print(f\"training set size: {len(train_set)} examples\")\n",
    "print(f\"testing set size: {len(test_set)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "            \n",
    "#         print(\"init_states\")\n",
    "#         print(f\"len(init_states): {len(init_states)}\")\n",
    "#         print(f\"init_states[0].shape: {init_states[0][0].shape}\")\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Se2 convLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMG_W = 48\n",
    "IMG_H = 63\n",
    "IMAGE_SIZE = (IMG_W, IMG_H)\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "LEARNING_RATE=0.005\n",
    "EPOCH = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = torch.ones(1, 4, 48, 63).to(device) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvLSTMCell(4, 4, (3,3), False).to(device) # input_dim, hidden_dim, kernel_size, bias\n",
    "decoder = ConvLSTMCell(4, 4, (3,3), False).to(device) # input_dim, hidden_dim, kernel_size, bias\n",
    "\n",
    "# optimizer\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_set, encoder, decoder):\n",
    "\n",
    "    losses = []\n",
    "    for i, train_imgs in enumerate(train_set):\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(\" . \", end=\"\")\n",
    "\n",
    "        train_imgs = train_imgs.squeeze(0)\n",
    "\n",
    "        # input seq\n",
    "        input_images = train_imgs[:72].to(device) # (72, 4, 48, 63)\n",
    "        # target seq\n",
    "        target_images = train_imgs[72:].to(device) # (48, 4, 48, 63)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        # initilize first hidden states of encoder\n",
    "        encoder_h, encoder_c = encoder.init_hidden(BATCH_SIZE, IMAGE_SIZE)\n",
    "\n",
    "        for idx in range(input_images.shape[0]):\n",
    "            input_img = torch.unsqueeze(input_images[idx], 0)\n",
    "            encoder_h, encoder_c = encoder(input_img, (encoder_h, encoder_c))\n",
    "\n",
    "        # out seq\n",
    "        output_images = torch.zeros((48, 4, 48, 63))\n",
    "\n",
    "        decoder_h, decoder_c = decoder(START, (encoder_h, encoder_c))\n",
    "\n",
    "        for idx in range(1, output_images.shape[0]):\n",
    "\n",
    "            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "\n",
    "                # Teacher Forcing: Feed the target as the next input\n",
    "                input_img = torch.unsqueeze(target_images[idx], 0)\n",
    "                decoder_h, decoder_c = decoder(input_img, (decoder_h, decoder_c)) # teacher forcing\n",
    "                loss += criterion(decoder_h, target_images[idx].unsqueeze(0))\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                decoder_input = decoder_h.detach() # detach from history as input\n",
    "                decoder_h, decoder_c = decoder(decoder_input, (decoder_h, decoder_c))\n",
    "\n",
    "            output_images[idx] = decoder_h\n",
    "            loss += criterion(decoder_h, target_images[idx].unsqueeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        losses.append(loss.item() / TIME_WINDOW)\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1/2\n",
      " .  .  . \n",
      "EPOCH: 2/2\n",
      " .  .  . \n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(EPOCH):\n",
    "    print(f\"EPOCH: {epoch+1}/{EPOCH}\")\n",
    "    loss = train(train_set, encoder, decoder)\n",
    "    losses += loss\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylabel('MSE Loss'), plt.xlabel('examples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1532241980234782"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[len(losses) - 10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
